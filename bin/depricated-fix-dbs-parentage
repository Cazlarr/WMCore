#!/usr/bin/env python
"""
WARNING: Please don't use this script. This ony use for testing purpose.
It will be removed when the testing is done.
"""
from __future__ import print_function, division
import json
import time
import httplib
import os
import datetime
import argparse

from pprint import pprint
from collections import defaultdict
from dbs.apis.dbsClient import DbsApi

def callRESTAPI(restURL, url="cmsweb.cern.ch"):
    conn = httplib.HTTPSConnection(url, cert_file=os.getenv('X509_USER_CERT'),
                                 key_file=os.getenv('X509_USER_KEY'))
    conn.request('GET', restURL, headers={"Accept": "application/json"})
    r2 = conn.getresponse()
    data = r2.read()
    s = json.loads(data)
    return s


def listOfParentDatasets(dataset):
    parents = dbs.listDatasetParents(dataset=dataset)
    parentDatasets = []
    for parent in parents:
        parentDatasets.append(parent['parent_dataset'])
    return parentDatasets

def listBlocksWithNoParents(dataset):
    allBlocks = dbs.listBlocks(dataset=dataset)
    blockNames = []
    for block in allBlocks:
        blockNames.append(block['block_name'])
    parentBlocks = dbs.listBlockParents(block_name=blockNames)

    cblock = set()
    for pblock in parentBlocks:
        cblock.add(pblock['this_block_name'])

    noParentBlocks = set(blockNames) - cblock
    return noParentBlocks

def listFilesWithNoParents(blockName):
    allFiles = dbs.listFiles(block_name=blockName)
    parentFiles = dbs.listFileParents(block_name=blockName)

    allFileNames = set()
    for fInfo in allFiles:
        allFileNames.add(fInfo['logical_file_name'])

    cfile = set()
    for pFile in parentFiles:
        cfile.add(pFile['logical_file_name'])

    noParentFiles = allFileNames - cfile
    return noParentFiles

def getParentFilesFromDataset(parentDataset, lfn):
    fInfo = dbs.listFileLumis(logical_file_name=lfn)
    pLFNsByBlocks = defaultdict(set)
    for f in fInfo:
        pFiles = dbs.listFiles(dataset=parentDataset, run_num=f['run_num'], lumi_list=f['lumi_section_num'])
        for fi in pFiles:
            blocks = dbs.listBlocks(logical_file_name=fi['logical_file_name'])
            for bl in blocks:
                pLFNsByBlocks[bl['block_name']].add(fi['logical_file_name'])
    return pLFNsByBlocks

def getParentFilesFromDatasetByLumi(parentDataset, lfn):
    fInfo = dbs.listFileLumis(logical_file_name=lfn)
    pFileSet = set()
    for f in fInfo:
        pFiles = dbs.listFiles(dataset=parentDataset, run_num=f['run_num'], lumi_list=f['lumi_section_num'])
        for fi in pFiles:
            pFileSet.add(fi['logical_file_name'])

    return len(pFileSet)

def getParentfilesMissingParents(dataset, searchFile=False):

    pDatasets = listOfParentDatasets(dataset)
    # print("parent datasets %s\n" % pDatasets)
    if not pDatasets:
        # print("No parents dataset found for %s\n" % dataset)
        return None, None

    if not searchFile:
        blocks = listBlocksWithNoParents(dataset)
    else:
        blocks = [b['block_name'] for b in dbs.listBlocks(dataset=dataset)]

    if blocks:
        # DatasetReport = {'child_dataset': dataset, 'parent_datasets': pDatasets, 'mssing_parent_blocks': []}
        DatasetReport = {'CDS': dataset, 'PDS': pDatasets, 'MISSING': []}
    else:
        return None, None

    print("=== searching problem dataset %s ===\n" % dataset)

    countChildFiles = 0
    countParentFiles = 0
    countChildBlocks = 0
    countParentBlocks = 0

    for blockName in blocks:
        # BlockLevel = {'child_block': blockName, "parent_blocks": set(), 'lfn_parentage': []}
        noParentFiles = listFilesWithNoParents(blockName)
        # print("%s block has %s files with no parents\n\n" % (blockName, len(noParentFiles)))
        if noParentFiles:
            BlockLevel = {'CBK': blockName, "PBK": [], 'PT': []}
            countChildBlocks += 1
        else:
            continue

        countChildFiles += len(noParentFiles)
        parentBlocks = set()
        for lfn in noParentFiles:
            # print("child file : %s\n" % lfn)
            parentlfns = []  # combined parent with different datasts
            for parentDataset in pDatasets:
                # print("found parents %s dataset" % parentDataset)
                plfns = getParentFilesFromDataset(parentDataset, lfn)
                # pprint("block, files %s" % dict(plfns))
                for pblock, pfiles in plfns.items():
                    # BlockLevel['parent_blocks'].append(pblock)
                    parentBlocks.add(pblock)
                    parentlfns.extend(list(pfiles))
                countParentFiles += len(parentlfns)
                # BlockLevel['lfn_parentage'].append({'child_lfn': lfn, 'parent_lfns': parentlfns})
            BlockLevel['PT'].append({'CLFN': lfn, 'PLFN': parentlfns})
        BlockLevel['PBK'] = list(parentBlocks)
        if searchFile:
            child_parent_already_exist = []
            originalBlocks = dbs.listBlockParents(block_name=blockName)
            caculatedBlocks = set(BlockLevel['PBK'])
            if len(caculatedBlocks) != len(originalBlocks):
                print("Block doesn't match %s - %s" % (len(caculatedBlocks), len(originalBlocks)))
                print("Block: %s" % blockName)
                originalSet = set()
                for block in originalBlocks:
                    print(block)
                    originalSet.add(block['parent_block_name'])
                print("Difference lumi-matching - original: %s" % (caculatedBlocks - originalSet))
                print("Difference original - lumi-matching: %s" % (originalSet - caculatedBlocks))

                common = (originalSet & caculatedBlocks)
                print("Intersection original and lumi-matching: %s" % common)
                for pBlock in common:
                    child_parent_already_exist.append({'cBK': blockName, 'pBK': pBlock})

        countParentBlocks += len(BlockLevel['PBK'])
        # DatasetReport['missing_parent_blocks'].append(BlockLevel)
        DatasetReport['MISSING'].append(BlockLevel)
        print("Already exist")
        print(child_parent_already_exist)

    print("Total %s blocks with no parent child blocks (%s) found, %s parent bocks, %s child files, %s parent_files\n" % (
            len(blocks), countChildBlocks, countParentBlocks, countChildFiles, countParentFiles))
    metaData = {"NumChildDatasets": 1, "NumParentDatasets": len(pDatasets),
                "NumChildBlocks": countChildBlocks, "NumParentBlocks": countChildBlocks,
                "NumChildFiles": countChildFiles, "NumParentFiles": countParentFiles, }

    print("====================================\n\n")

    return [DatasetReport], metaData


def getParentFilesUsingLumi(dataset):
    """
    use only for the perfomance test
    :param dataset:
    :return:
    """
    pDatasets = listOfParentDatasets(dataset)
    # print("parent datasets %s\n" % pDatasets)
    if not pDatasets:
        # print("No parents dataset found for %s\n" % dataset)
        return None, None

    blocks = [b['block_name'] for b in dbs.listBlocks(dataset=dataset)]

    countChildFiles = 0
    countParentFiles = 0

    for blockName in blocks:
        allFiles = dbs.listFiles(block_name=blockName)
        noParentFiles = set()
        for fInfo in allFiles:
            noParentFiles.add(fInfo['logical_file_name'])

        countChildFiles += len(noParentFiles)

        for lfn in noParentFiles:
            for parentDataset in pDatasets:
                # print("found parents %s dataset" % parentDataset)
                pCount = getParentFilesFromDatasetByLumi(parentDataset, lfn)
                countParentFiles += pCount

    metaData = {"NumChildDatasets": 1, "NumParentDatasets": len(pDatasets),
                "NumChildFiles": countChildFiles, "NumParentFiles": countParentFiles, }

    return metaData

def getMissingParentSummaryUsingBlock(datasetList):
    totalMetaData = {"NumChildDatasets": 0, "NumParentDatasets": 0,
                "NumChildBlocks": 0, "NumParentBlocks": 0,
                "NumChildFiles": 0, "NumParentFiles": 0, }

    missingParents = []
    start = int(time.time())
    for dataset in datasetList:
        if isinstance(dataset, dict):
            dsName = dataset['dataset']
        else:
            dsName = dataset
        datasetInfo, metaData = getParentfilesMissingParents(dsName)

        if datasetInfo is not None:
            missingParents.append(datasetInfo)
            for k in totalMetaData:
                totalMetaData[k] += metaData[k]

    cEnd = int(time.time())

    with open("./missing_parents.json", "w") as f:
        json.dump(missingParents, f, indent=4)
    jEnd = int(time.time())

    with open("./missing_parents.py", "w") as f:
        f.write("MD = %s" % missingParents)

    pEnd = int(time.time())
    print("dbs call time %s" % (cEnd - start))
    print("json create time %s" % (jEnd - cEnd))
    print("python create time %s" % (pEnd - jEnd))
    print("total meta data:\n")
    pprint(totalMetaData)
    return missingParents


def getMissingParentSummaryUsingFile(dataset):
    missingParents, metaData = getParentfilesMissingParents(dataset, True)
    with open("./missing_parents_by_file.json", "w") as f:
        json.dump(missingParents, f, indent=4)
    with open("./missing_parents_by_file.py", "w") as f:
        f.write("MD = %s" % missingParents)
    return [missingParents]


def getOutputDSFromActiveWFs():
    restURL = '/wmstatsserver/data/filtered_requests?mask=OutputDatasets&mask=RequestType'
    result = callRESTAPI(restURL)["result"]

    outDataDict = {}
    for info in result:
        for outputDS in info["OutputDatasets"]:
            outDataDict.setdefault(outputDS, [])
            outDataDict[outputDS].append((info["RequestType"], info["RequestName"]))
    return outDataDict


def investigateACDCCollection(workflow):
    restURL = '/couchdb/acdcserver/_design/ACDC/_view/byCollectionName?key="%s"&reduce=false&include_docs=true' % workflow
    result = callRESTAPI(restURL)
    missingFiles = set()
    if "rows" in result:
        if result["rows"]:
            for row in result["rows"]:
                for inFile, value in row["doc"]["files"].items():
                    if not value["merged"] or value["merged"] == "0":
                        if value["parents"] and "/unmerged/" in value["parents"][0]:
                            missingFiles.add(inFile)
                        elif value["parents"]:
                            parentFiles = dbs.listFileArray(logical_file_name=value["parents"])
                            if len(parentFiles) != len(value["parents"]):
                                fromDBS = set([x["logical_file_name"] for x in preantFiles])
                                notInDBS = set(value["parents"]) - fromDBS
                                print(notInDBS)
                                exit(1)
        return missingFiles
    else:
        print(result)
        return missingFiles

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument("start", help="start date for dataset search DD/MM/YYYY format")
    parser.add_argument("end", help="end date for dataset search DD/MM/YYYY format")

    args = parser.parse_args()
    start = time.mktime(datetime.datetime.strptime(args.start, "%d/%m/%Y").timetuple())
    end = time.mktime(datetime.datetime.strptime(args.end, "%d/%m/%Y").timetuple())

    parser.add_argument('--production', dest='prod', action='store_true')
    parser.add_argument('--testbed', dest='prod', action='store_false')
    parser.set_defaults(prod=False)

    args = parser.parse_args()

    if args.prod:
        dbsURL = 'https://cmsweb.cern.ch/dbs/prod/global/DBSWriter'
        print("Using production: ", dbsURL)
    else:
        dbsURL = 'https://cmsweb-testbed.cern.ch/dbs/int/global/DBSWriter'
        print("Using testbed: ", dbsURL)
    
    dbs = DbsApi(dbsURL)

    start = time.mktime(datetime.datetime.strptime(args.start, "%d/%m/%Y").timetuple())
    end = time.mktime(datetime.datetime.strptime(args.end, "%d/%m/%Y").timetuple())

    datasetList = dbs.listDatasets(min_cdate=int(start), max_cdate=int(end))
    print(len(datasetList))

    for dataset in datasetList:
        if isinstance(dataset, dict):
            dsName = dataset['dataset']
        else:
            dsName = dataset
        for parentDataset in listOfParentDatasets(dsName):
            print("Testing dataset: %s parent dataset:%s" % (dsName, parentDataset))
            sTime = int(time.time())
            print(getParentFilesUsingLumi(dsName))
            eTime = int(time.time())
            print("Total time: ", eTime - sTime)
            break

    #outDS = getOutputDSFromActiveWFs()
    #datasetList = outDS.keys()
    #missingPS = getMissingParentSummaryUsingBlock(datasetList)

    #for info in missingPS:
    #    cDS = info['CDS']
        #print("%s: %s" % (cDS, outDS[cDS]))

    #wfs = set()
    #for ds, value in outDS.items():
    #    for reqInfo in value:
    #        if reqInfo[0] != "Resubmission":
    #            wfs.add(reqInfo[1])
    #missingFiles = set()
    #for wf in wfs:
    #    missingFiles = missingFiles.union(investigateACDCCollection(wf))

    #with open("./child_with_umerged_parents.json", "w") as f:
    #    json.dump(list(missingFiles), f, indent=4)

    #print(len(missingFiles))

    #getMissingParentSummaryUsingFile("/SingleMuon/Run2016D-03Feb2017-v1/MINIAOD")


    #sTime = int(time.time())
    #print(getParentFilesUsingLumi("/SingleMuon/Run2016D-03Feb2017-v1/MINIAOD"))
    #eTime = int(time.time())

    #print("Total time: ", eTime - sTime)